{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\jy3.6\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "Episode 1 with Reward : 2337.5960434355693 at epsilon 0.9001999999999335 in steps 7023\n",
      "target parameters changed\n",
      "Episode 2 with Reward : 118.5270990155332 at epsilon 0.9001999999999335 in steps 789\n",
      "target parameters changed\n",
      "Episode 3 with Reward : 63.573471340625815 at epsilon 0.9001999999999335 in steps 231\n",
      "Episode 4 with Reward : 66.95093782494139 at epsilon 0.9001999999999335 in steps 208\n",
      "target parameters changed\n",
      "Episode 5 with Reward : 96.14738936523169 at epsilon 0.9001999999999335 in steps 317\n",
      "target parameters changed\n",
      "Episode 6 with Reward : 105.72826140334634 at epsilon 0.9001999999999335 in steps 488\n",
      "target parameters changed\n",
      "Episode 7 with Reward : 128.79405152866275 at epsilon 0.9001999999999335 in steps 483\n",
      "Episode 8 with Reward : 100.37082729952321 at epsilon 0.9001999999999335 in steps 347\n",
      "target parameters changed\n",
      "Episode 9 with Reward : 55.85518734033461 at epsilon 0.9001999999999335 in steps 167\n",
      "Episode 10 with Reward : 55.97312458913839 at epsilon 0.9001999999999335 in steps 172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e906fcd46dd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\jy3.6\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcartrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keys_to_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\jy3.6\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mglClearColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\jy3.6\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No GL context; create a Window first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gl_begin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglGetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgluErrorString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deep Q-Network: Mountain Car에 적용한 DQN 예제\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, learning_rate, gamma, n_features, n_actions, epsilon, parameter_changing_pointer, memory_size):\n",
    "        self.learning_rate = learning_rate  #학습률\n",
    "        self.gamma = gamma            #할인율\n",
    "        self.n_features = n_features  #자동차의 위치(0)와 속도(1) = state\n",
    "        self.n_actions = n_actions    #왼쪽으로 밀기(0), 보류(1), 오른쪽으로 밀기(2) \n",
    "        self.epsilon = epsilon        #탐욕 정책 시 활용되는 탐욕의 초기 값       \n",
    "        self.batch_size = 100         #재생 메모리로부터 추출되는 표본의 크기\n",
    "        self.experience_counter = 0   #현재 재생 메모리에 저장된 표본의 수\n",
    "        self.experience_limit = memory_size  #재생 메모리의 최대 용량\n",
    "        self.replace_target_pointer = parameter_changing_pointer  #target network 갱신 기준 학습 단계\n",
    "        self.learning_counter = 0                                 #primary network의 학습 단계\n",
    "        self.memory = np.zeros([self.experience_limit, self.n_features*2+2])  #재생 메모리의 초기값  \n",
    "                                                                              # (5000x(2*2+2)) = (5000, 6)\n",
    "        self.build_networks() #primary network과 target network을 생성\n",
    "        p_params = tf.get_collection('primary_network_parameters')\n",
    "        t_params = tf.get_collection('target_network_parameters')\n",
    "        self.replacing_target_parameters = [tf.assign(t,p) for t,p in zip(t_params,p_params)] # target의 변수에 p넷 변수 입력\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#========== DQN 모형의 기본 신경망 및 목표 신경망을 설정하는 단계 ============#\n",
    "\n",
    "    def build_networks(self):\n",
    "        hidden_units = 10 # 노드 10개\n",
    "#.....................................................................#\n",
    "        # Primary Network: 각 10개의 은닉노드를 갖는 2개의 은닉층 , 최적의 a 찾음\n",
    "        self.s = tf.placeholder(tf.float32,[None,self.n_features]) # 입력값은 (None=100,2) : 위치, 속도 = state\n",
    "        self.qtarget = tf.placeholder(tf.float32,[None,self.n_actions]) # 입력값은 (None,2) : 왼쪽, 보류, 오른쪽\n",
    "\n",
    "        with tf.variable_scope('primary_network'): #변수 볌위를 관리한다. (네트워크의 변수 정의)\n",
    "            c = ['primary_network_parameters', # primary_network_parameters라는 이름으로 네트워크의 변수를 c에 저장?\n",
    "\t\t\t\t\ttf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            # first layer: 상태 학습\n",
    "            with tf.variable_scope('layer1'): # 층 파라미터 설정\n",
    "                w1 = tf.get_variable('w1', [self.n_features, hidden_units], # s, shape=[2,10]\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(), # s의 가중치 초기값 설정\n",
    "                                     dtype=tf.float32,collections=c) # 이 변수는 c에 저장됨\n",
    "\n",
    "                b1 = tf.get_variable('b1', [1, hidden_units], # s의 가중치 bias 변수, shape=[1,10]\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(), # bias 초기값 설정\n",
    "                                     dtype=tf.float32,collections=c) # 이 변수는 c에 저장됨\n",
    "\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1) # relu로 state 선형결합 = 예측된 s\n",
    "\n",
    "            # second layer: 행동 학습\n",
    "            with tf.variable_scope('layer2'):\n",
    "                w2 = tf.get_variable('w2', [hidden_units, self.n_actions], # action, shape=[10,3]\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c) # 이 변수는 c에 저장됨\n",
    "\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions],# action의 가중치 bias 변수, shape=[1,3]\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c) # 이 변수는 c에 저장됨\n",
    "\n",
    "                self.qeval = tf.matmul(l1, w2) + b2 # relu로 action 선형결합(계산)\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "                self.loss = tf.reduce_mean(tf.squared_difference( \n",
    "\t\t\t\t\t\tself.qtarget, self.qeval))\n",
    "        with tf.variable_scope('optimiser'):\n",
    "                self.train = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "#.....................................................................#\n",
    "        # Target Network(최적의 a를 갖고 q를 계산)\n",
    "        self.st = tf.placeholder(tf.float32,[None,self.n_features]) # 입력형태 [None,2]\n",
    "\n",
    "        with tf.variable_scope('target_network'): # 네트워크 변수 정의\n",
    "            c = ['target_network_parameters', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            # first layer\n",
    "            with tf.variable_scope('layer1'): # 층의 변수 졍의\n",
    "                w1 = tf.get_variable('w1', [self.n_features, hidden_units], # st, shape=[2,10]\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                b1 = tf.get_variable('b1', [1, hidden_units], # [1,10]\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                l1 = tf.nn.relu(tf.matmul(self.st, w1) + b1) # [None, 10]\n",
    "\n",
    "            # second layer\n",
    "            with tf.variable_scope('layer2'):\n",
    "                w2 = tf.get_variable('w2', [hidden_units, self.n_actions], # action, shape=[10,3]\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions],\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                self.qt = tf.matmul(l1, w2) + b2\n",
    "\n",
    "#-----------------------------------------------\n",
    "#기본 신경망의 학습 결과를 목표 신경망에 대입하기 위한 세션을 구성한다\n",
    "    def target_params_replaced(self):\n",
    "        self.sess.run(self.replacing_target_parameters)\n",
    "\n",
    "    def store_experience(self,obs,a,r,obs_):\n",
    "        index = self.experience_counter % self.experience_limit\n",
    "        self.memory[index,:] = np.hstack((obs,[a,r],obs_))\n",
    "        self.experience_counter+=1\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "#재생 메모리에 저장된 과거 경험을 설정한 배치 크기만큼 랜덤으로 추출하여 학습 데이터 집합으로 설정한다.\n",
    "    def fit(self):\n",
    "        # sample batch memory from all memory\n",
    "        if self.experience_counter < self.experience_limit: # 현재 메모리에 저장된 샘플 수 < 최대용량\n",
    "            indices = np.random.choice(self.experience_counter, size=self.batch_size) # 100개 랜덤 선택\n",
    "        else: \n",
    "            indices = np.random.choice(self.experience_limit, size=self.batch_size)\n",
    "\n",
    "        batch = self.memory[indices,:] # 5000행 중의 한 행\n",
    "        qt,qeval = self.sess.run([self.qt, self.qeval], #  self.qeval = tf.matmul(l1, w2) + b2 \n",
    "                                                        #  self.qt = tf.matmul(l1, w2) + b2\n",
    "\tfeed_dict={self.st:batch[:,-self.n_features:], self.s:batch[:,:self.n_features]})\n",
    "\n",
    "        qtarget = qeval.copy() #  \n",
    "        batch_indices = np.arange(self.batch_size, dtype=np.int32) # [0, ,,, batch_size-1]\n",
    "        actions = self.memory[indices,self.n_features].astype(int) # \n",
    "        rewards = self.memory[indices,self.n_features+1] # \n",
    "        qtarget[batch_indices,actions] = rewards + self.gamma * np.max(qt,axis=1) # \n",
    "\n",
    "        _ = self.sess.run(self.train,feed_dict = {self.s:batch[:,:self.n_features],\n",
    "\t\t\t\t\t\t\tself.qtarget:qtarget})\n",
    "        if self.epsilon < 0.9:\n",
    "            self.epsilon += 0.0002\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "#학습 시 기본 신경망의 가중치를 가져와 목표 신경망의 가중치를 갱신한다.\n",
    "        if self.learning_counter % self.replace_target_pointer == 0:\n",
    "            self.target_params_replaced()\n",
    "            print(\"target parameters changed\")\n",
    "        self.learning_counter += 1\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "#탐욕 정책을 통해 행동을 선택하는 함수를 정의한다.\n",
    "    def epsilon_greedy(self,obs):\n",
    "        #epsilon greedy implementation to choose action\n",
    "        if np.random.uniform(low=0,high=1) < self.epsilon:\n",
    "            return np.argmax(self.sess.run(self.qeval,\n",
    "\t\t\t\t\tfeed_dict={self.s:obs[np.newaxis,:]}))\n",
    "        else:\n",
    "            return np.random.choice(self.n_actions)\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "#DQN 객체를 생성해 에이전트를 학습시키고 결과를 도출하는 단계\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env = env.unwrapped\n",
    "    dqn = DQN(learning_rate=0.001, gamma=0.9, n_features=env.observation_space.shape[0], \t n_actions=env.action_space.n, \n",
    "              epsilon=0.0, parameter_changing_pointer=500,  memory_size=5000)\n",
    "\n",
    "    episodes = 10\n",
    "    total_steps = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        steps = 0\t\t\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            env.render()\n",
    "            action = dqn.epsilon_greedy(obs)\n",
    "            obs_,reward,terminate,_ = env.step(action)\n",
    "            reward = abs(obs_[0]+0.5)\n",
    "            dqn.store_experience(obs,action,reward,obs_)\n",
    "            if total_steps > 1000: # total_step= 1000부터 학습\n",
    "                dqn.fit()\n",
    "            episode_reward+=reward\n",
    "            if terminate:\n",
    "                break\n",
    "            obs = obs_\n",
    "            total_steps+=1\n",
    "            steps+=1\n",
    "        print(\"Episode {} with Reward : {} at epsilon {} in steps {}\".\n",
    "\t\t\tformat(episode+1,episode_reward,dqn.epsilon,steps))\n",
    "\n",
    "    while True:  \n",
    "        env.render()\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jy3.6",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
